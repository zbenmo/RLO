{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reinforcement Learning Observations","text":"<p>Welcome to my RL related blog.</p> <p>We'll review reinforcement-learning libraries and concepts, and strive to keep track of the state-of-the-art in the RL arena, and to suggest the best engineering practices.</p> <p>We'll talk here about environments and about agents. We'll make observations and we will be rewarded!</p>"},{"location":"reinforcement_learning/","title":"Reinforcement Learning","text":"<p>We want to build software that automates tasks or acts on our behalf in various situations. This is nothing new. We have build software for some time now. We have came up with algorithms and engineering solutions.</p> <p>The future lies apparently with machine learning. We can build parts of our software systems in new ways. By training software from examples. So instead of figuring how to do stuff, we communicate to the software what we want, and the software is built automatically from this description.</p> <p>For example for teaching software to distinguish between cats and dogs in images we provide examples of cats and of dogs and we use supervised learning. This is a machine learning technique with which a model slowly picks up the statistical hints from the labeled examples till it can fulfil the task of predicting correctly the labels on unseen examples, and hence we have achieved a software solution that can carry on from now in telling dogs from cats (and humans can enjoy and supervise while the machines do the hard work).</p> <p>Reinforcement learning is a similar concept. We want software that can play Chess against us, or control a self-driving car, and more. Note that here there are multiple steps. Not just a single decision cat or dog, but rather a series of actions. The software \u201cwins\u201d only if all its moves led to a victory. The car needs to get safely to the destination, following the traffic rules and in a reasonable time. Accelerating, slowing, changing the wheel orientation, all need to contribute for this complex task.</p> <p>To some degree we can employ again supervised learning. In each situation, we can label the desired actions, for example by recording the actions of a human driver. We can then train a model to interpolate desired actions in similar situations. There are many Chess games to learn from, we can try to train a model based on those games. </p> <p>Reinforcement learning is a complementary technique to train models or here we\u2019ll call those agents. Reinforcement learning may be more suitable for those control settings. For example with Chess a move is actually a part of a plan. Learning to repeat a move without gasping what needs to be done later may not work without extensive training set that may not be available, or is too hard to achieve with supervised training. The approach with reinforcement learning is to let the agent learn from interacting with the environment.</p> <p>The environment produces observations and the actions of the agent are communicated to the environment. The environment changes and the agent needs to learn to make smart decisions about its actions. The agent learns this time not from labels but rather by a reward mechanism that we provide to map a transition from one state1 in the environment to another state and its consequences.</p> <p>The agent should learn a policy to map an observation to the best action as to be rewarded as much as possible during the episode. In other words, the agent shall attempt to maximize the cumulative reward to some time horizon. For example the agent shall try to learn to win the Chess game, or to bring the passengers safely to their destination, while avoiding fines and in a reasonable time. There is much to talk about reward and how to design the reward mechanism. If in supervised learning one needs to make sure the labels are correct here with reinforcement learning, the reward mechanism is the equivalent.</p> <p>Note that the environment replaces the training set. By interacting with the environment, the agent can receive infinite feedback. There are some challenges however. We don\u2019t want a car to self-drive in the real world before it is ready. We should have some simulator. The simulator should be very close to the real world and should reflect what happens in the real world. The reward mechanism that is often also associated with the environment should be relevant to learning the desired behavior for the agent.</p> <p>Another very important observation here is that the experience that the agent collects depends on the agent's actions. If a robot keeps going to the left, it will never find out what is there on the right. While in supervised learning we expect the samples in the training set to be of the same distribution (identically independently distributed), the interaction of an agent with an environment are very much dependent on the agent's actions and hence cannot be assumed to be of the same distribution.</p> <ol> <li> <p>A state may mean the same thing as an observation. In many settings the observation is a (partial) derivative of the state, in the sense that the environment's state is more complete and more accurate than the observation that is available to the agent.\u00a0\u21a9</p> </li> </ol>"},{"location":"Gymnasium/creating_a_new_environment/","title":"Creating a new Gymnasium environment","text":"<p>Developing a new Gymnasium / Gym environment involves subclassing gym,Env (potentially you declared <code>import gymnasium as gym</code>). You need to implement the following:</p> <ul> <li>reset</li> <li>step</li> <li>render</li> </ul> <p>In addition, your environment should have the following two attributes / member variables: observation_space and observation_space. For example in the initialization of your environment you can have (copied from Gymnasium documentation):</p> <pre><code>    ...\n    self.observation_space = spaces.Dict(\n        {\n            \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n            \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n        }\n    )\n\n    # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n    self.action_space = spaces.Discrete(4)\n    ...\n</code></pre> <p>'reset' should bring the environment to some initial state. It might involve randomness if relevant, or just to the starting position of, for example Chess game. 'reset' returns the initial observation.</p> <p>For Gymnasium, the signature of 'reset' looks as follows:</p> <pre><code>def reset(self, seed=None, options=None):\n  ...\n</code></pre> <p>It is a good idea to pass the 'seed' to 'super' even if no randomness is expected, to avoid environment checks failure.</p> <pre><code>def reset(self, seed=None, options=None):\n  # We need the following line to seed self.np_random\n  super().reset(seed=seed)\n  ...\n</code></pre> <p>As mentioned above 'reset' should return the first observation and I'm adding here, that also a \"free form\" info. For example:</p> <pre><code>def reset(self, seed=None, options=None):\n  ...\n  return self._get_obs(), self._get_info()\n</code></pre> <p>It is a good idea to \"wrap\" the logic of constructing the observation from the state in a function as you'll need it also in 'step'. The information output can be just an empty dict, <code>{}</code>, if you wish.</p> <p>'step' receives and action and should return a tuple with the following: the next observation, the reward, was the environment terminated, was the environment truncated, and again some info.</p> <p>So for example:</p> <pre><code>def step(self, action):\n  ...\n  reward = 1\n  done = False\n  info = {'mask': [True, False, False, True]} # an interested agent can use this mask to avoid illegal moves\n  return self._get_obs(), reward, done, False, info\n</code></pre> <p>'render' should for example nicely print the environment's state to the console.</p> <p>My suggestion would be to develop your environment without thinking about RL nor about Gym. Let's call this environment the \"native\" environment. Try to make your \"native\" environment passive in the sense, that in order to use it, one needs to call the methods of the environment from a loop. I've implemented an environment I'm calling \"Collect Coins\". See in qwertyenv.</p> <p>My environment is a turns game, two players, similar to Chess. The aim of the game is to collect more coins from the board than the other player. I've started by creating the \"game\" in a separate Python file. This is not a Gym environment, yet it enabled me to \"play\" in turn the white and then the black and so on till the game is over. I've added a check that you are playing the right player when you call the \"play\" function. Added also a verification that the play is legal. I have no reward mechanism other than allowing the user of the \"game\" to see the board, the current counts of coins, and if the game is done, who won.</p> <p>Now, to make it into an environment, I had to do the following:</p> <ul> <li>Keep an instance of the \"game\" in my environment. In 'reset' I could opt to create a new instance, or to call the equivalent on the self.game instance. I created a new instance of the \"game\" on each 'reset'.</li> <li>Find the Gym spaces that can be mapped to my \"game\" board and status, and the possible moves. For example, in my game, the board is a boolean 8x8 matrix where you still have coins, and in addition you have the location of the player and the location of the other player. Those three pieces of observation went into gym.spaces.Dict. Coming to think of it, an agent may wish to see the count of coins. I did not made this freely available for the agent in a straight manner in this version.</li> <li>The action space, I've decided to have as the target square. The reason is that in my game you can have various Chess pieces (this is determined in the initialization of the game). I have at the moment knights and rocks. The rocks are actually a limited version, where they can be moved only to a neighbour square. Maybe I'll add in the future also a \"real\" rock. By having an action space of the target square, I've avoided having two different action spaces for each. I'm just telling what I've done, don't take from that that it was a smart decision.  </li> <li>Develop a reward mechanism. In my first iteration I went for +1 for a win, 0 for draw, and -1 for a loss. </li> <li>Now I had an engineering challenge. Let's say I let the external agent act as the first player (white). But where will the other player come from? This had to be implemented in the environment. So towards the end of the 'step' method, I've took a move for the other player. In the 'reset' this logic was also used if the external agent is the black and needs to move second.</li> <li>The 'render' functionality was implemented in the \"native\" game environment.</li> </ul> <p>If you build a Python package, it is a good practice to 'register' your Gym environment, so that one can later find it for example with <code>env =  gym.make('qwertyenv/CollectCoins-v0', pieces=['rock', 'rock'])</code></p> <p>A registration is done usually in the __init__.py of the package:</p> <pre><code>from gym.envs.registration import register\n\n...\nregister(\n    id='qwertyenv/CollectCoins-v0',\n    entry_point='qwertyenv.collect_coins:CollectCoinsEnv',\n    max_episode_steps=300\n)\n...\n</code></pre> <p>Here are some lessons I learned.</p> <p>Make sure to debug the \"native\" environment (asserts, pytest). </p> <p>For your Gym environment, write as a minimum, a pytest to call:</p> <pre><code>from gymnasium.utils.env_checker import check_env\n..\n\n...\n\ndef test_my_env():\n  ..\n  check(env)\n</code></pre> <p>The observation_space and the action_space, have in addition to \"sample\", also a very useful functionality for debugging. One can check if the space \"contains\" a specific value that he/she wants to return as an observation, or pass as an action. When the relevant space does not contain the value, this must be first solved by either changing the space, or realizing that the action or observation indicate a potential bug.</p> <p>Above enabled me to test with SB3 (I wrote it for Gym '0.21.0'). I will tell more about my experiments when discussing Gym wrappers and PettingZoo (I will write about it soon......)</p>"},{"location":"Gymnasium/gym/","title":"Gym / Gymnasium","text":"<p>When we dive into RL we often discover that the environment is at least as important as the exact algorithm. Not to say that all algorithms are suitable in all situations, but most of us will probably use an existing RL library and hope for the best.</p> <p>The environment on the other hand is where we\u2019ll spend our time to match it to the task we want to achieve. Gym / Gymnasium (Open AI and from now Farama foundation) is currently the de-facto standard for Python environments. A lot of RL Python packages for agents / policies are dependent on Gym. As an example, \u2018stable-baselines3\u2019 (sb3 for short), is dependent on 'torch' but also on \u2018gym\u2019 (and on a few more packages).</p> <p>According to Gym, an environment has the following functionality:</p> <ul> <li> <p>Initial_obs = reset()</p> </li> <li> <p>obs, reward, done, info = step(action)</p> </li> <li> <p>render()</p> </li> </ul> <p>In addition the environment defines the \u2018observation_space\u2019 and the \u2018action_space\u2019. An agent intended to be used with a specific Gym environment, should be adjusted to the relevant \u2018observation_space\u2019 and the relevant \u2018action_space\u2019. Such an agent should provide the functionality to select an action given an observation. The learning setting involves loops where the observations, actions, rewards, and new observations are taken into account to update the policy of the agent based on the relevant RL learning algorithm.</p> <p>Note that the time according to Gym is discrete, in each time step the agent decides on an action based on the current observation and this action is fed into the environment\u2019s \u2018step\u2019 function.</p> <p>The Gym environment is intended for training a single agent. A lot of times this is just what we need. Even if, for example the environment represents a Chess game, and the agent represents the white, then the agent gets a chance to make the first move, then the environment also takes the action for the black. The new observation shall include both moves. Assume for now that we already have some good Chess software that can be used by the environment to play the opponent.</p> <p>If we do want to have multiple agents for a turn-play, or for a simultaneous play with multiple agents, there is an extension of Gym called \u2018PettingZoo\u2019. In PettingZoo, one have an iterator over all the agents and is expected to call 'step' for each.</p> <p>The current state-of-the-art is a bit of a mess. Gym environment\u2019s API was modified so that \u2018step\u2019 now returns instead of \u2018done\u2019, two new outputs: \u2018terminated\u2019, \u2018truncated\u2019. To indicate what exactly happened, whether we reached the \u201cgoal\u201d or we\u2019ve passed the allowed number of steps.</p> <p>Gym \u20180.21.0\u2019 is still with the \u2018done\u2019. After that version we have:</p> <ul> <li>obs, reward, terminated, truncated, info = step(action)</li> </ul> <p>On top of that Open AI donated the code to Farama foundation, and the name was changed to Gymnasium. So you\u2019ll sometimes in Python codes \u2018import gymnasium as gym\u2019. When you start playing with RL, for example with \u2018stable-baselines3\u2019, you most likely want to make sure to use Gym with version \u20180.21.0\u2019 or what sb3 depends on at the time.</p> <p>If you want to create a new environment to represent your challenge, you can consider working with Gymnasium, yet make sure you have a matching RL package / code. You can use Tianshou or you can copy a Python (one file) implementation from CleanRL and do the relevant adjustments.</p> <p>I want to add a side note that I find Gym environment suggested API and in fact the standard, to leave the taste of something missing. I find it horrifying that the existing RL packages are forced to make Gym to be their dependency. A better setting, for me, would be a loosely coupling between the package for the environment and the package for the RL agents / algorithms / training.</p> <p>At the moment Gym / Gymnasium is here to stay, and you must wrap your head around it, that this is the way it works. There are benefits. All RL packages adjust their API to Gym environments so you can quickly try them out on your favorite environment.</p>"},{"location":"Gymnasium/spaces/","title":"Spaces","text":"<p>Gym environment defines observation space and action space. Think of those as the \u201ctypes\u201d of the input and the output for the agent\u2019s decision. The spaces also communicate the range or set of possible values. </p> <p>This makes a lot of sense as when one develops an agent to interact with that environment, the agent needs to be designed to expect observations of, for example images / pixels (ex. an ATARI game), or the output of some sensor, for example representing an angle. The actions of the agent may themselves be a discrete choice from a finite (small) set of possible values, as an example, yet it can also be a collection of values.</p> <pre><code>self.action_space = spaces.Discrete(3)\nself.observation_space = spaces.Box(0, 1, shape=(2,))\n</code></pre> <p>In above example, the action space is 3 possible actions, {0, 1, 2}, standing for example for: {left, right, none}. It is up to the environment to translate the given integer number from the set to the matching action. The observation space in the example above is a vector of two entries, each taking a value from [0, 1]. You can imagine it as a location of a joystick in a 1 by 1 squared box.</p> <p>The reward is a single 'float' number. \u2018truncated\u2019 and \u2018terminated\u2019 (or \u2018done\u2019) are of type 'bool'. There is also a general purpose output from the \u2018step\u2019 function (and also from 'reset'), that is referred to as \u2018info\u2019 and is a dictionary with whatever values the environment wants to communicate to the outside world in addition to the fixed contract given by the observation space and the action space.</p> <p>In the example of a Chess game, we may want to communicate to the agent where are its pieces, where are the other player\u2019s pieces, and in this way unify the cases when the player is the white and when it is actually the black. In Chess we may also want to communicate if specific moves, such as castling, are still valid.</p> <p>What would be the action space for a Chess agent? A na\u00efve approach can be to pick a piece (a square), and move it to another square. A lot of those \u201cactions\u201d are just moving air. A lot of them are trying to move the opponents pieces, or to move a piece of yours to an illegal place. Yet still by this approach of choosing two squares, we may make a simple interface to an RL agent implementation.</p> <p>And so spaces are both the type of the inputs and the outputs, and also the possible values that are accepted. Is it a finite discrete choice, ex. {up, right, down, left}, or two numbers, each of which from [-3.1, 3.8], etc. One can define spaces that are a combination of values of complimentary meaning, values, and types. The combinations are defined, among other options, with Dict spaces.</p> <p>The environment should define its observation space and its action space based on its needs, however, once defined, we need to find RL algorithm implementation that supports the required observation and action spaces. </p> <p>For example, if the environment wish to provide images as observations, we may need an RL implementation that have maybe a CNN. If the action space is discrete we may have the output to be a fixed size layer with softmax activation. Dict observations may require some software construct that shall combine those in some clever way to the inputs of a NN for example.</p> <p>A nice feature of spaces is that you can sample from them. Which means that you can play with an environment, even before having any agent. For example (taken from Gymnasium documentation):</p> <pre><code>import gymnasium as gym\nenv = gym.make(\"LunarLander-v2\", render_mode=\"human\")\nobservation, info = env.reset()\n\nfor _ in range(1000):\n    action = env.action_space.sample()  # agent policy that uses the observation and info\n    observation, reward, terminated, truncated, info = env.step(action)\n\n    if terminated or truncated:\n        observation, info = env.reset()\n\nenv.close()\n</code></pre> <p>Note that sampling from the observation space may return \"images\" that should never actually be returned from the environment (ex. complete random noise). That is because the observation space is wider than the actual distribution of values the agent will actually see. The action space may also be wider than the legal actions. If you stample on such an issue, you may want to read about action masking.</p> <p>Another item to make our lives even a bit more interesting, is for example a game like Contract Bridge where you have multiple stages. In Contract Bridge we have the bidding, and the rest of the game. The observation space as well as the action space of an environment are fixed. It is up to you to come up with a setting that can handle all the stages. For example by having an agent for stage, each interacting with one environment dedicated for that stage, or by concatenating somehow the action apaces, and adding in the observation some hint in which stage we are. Or also as mentioned above by using action masking.</p> <p>As far as I know, most environments do not allow for setting their state explicitly, but rather they either start from the same point (ex. a Chess game), or start from a random state (ex. a Contract Bridge cards shuffle). If relevant, you may hack your environment to start from a specific state, this would be just software engineering.</p>"},{"location":"Gymnasium/wrappers/","title":"Wrappers","text":"<p>A wrapper in Gym / Gymnasium is just an environment that delegates its calls to another environment, potentially doing first some adjustments. Why is it a good idea and when will we be using wrappers?</p> <p>The agents developed against Gym environments in mind, expect a specific interface. That is in particular the 'step' function. One is expected to pass as an argument to the 'step' function an action from the action space and one expects to receive in return an observation (the next observation) from the observation space. The reward that is returned from the 'step' function determines also if the agent is able to learn the desired behaviour, and is not less important from the observation and the learning algorithm itself.</p> <p>What do you do when you want your agent to think of the problem a little different. What do you do if you want the action space to be a bit different? I'll give an example. The environment expects a target destination on an '8x8' board game. You train an agent with your preferred RL library, and a suitable method, yet the agent seems to learn very slow. Let's assume that the agent avoids illegal moves, for example, by taking into account action masking.</p> <p>Still the \"learning\" is slow. You brainstorm with friends and they point that your (agent) piece can only move {up, left, down, right}. You hypnotise that if the action space is changed to those four possible actions, the agent will learn faster. To try you quickly wrap the original environment, with an environment that declares its \u2018action_space\u2019 with above four directions, and translates this action into the target square. That's it. You can train an agent with the \"new\" environment and see if the learning is faster.</p> <p>While you can create a new environment to wrap the old environment, or also to create a new general wrapper based on gym.Wrapper, in above case you can actually base your wrapper on gym.ActionWrapper. If you base your wrapper on gym.ActionWrapper, you only need to have the new action_space available and to implement a function called action that takes an action from the new action space (the one of the wrapper), and return the matching action of the wrapped environment. For example, something I did in qwertyenv.</p> <pre><code>import gym\nfrom typing import Tuple, Callable\nimport numpy as np\n\n\nclass UpDownLeftRight(gym.ActionWrapper):\n  \"\"\"\n  A gym environment wrapper to enable U/D/L/R action space when the wrapped environment\n  actually expects a destination in cartesian coordinates.\n  \"\"\"\n\n  def __init__(self, env: gym.Env,\n               get_current_location: Callable[[], Tuple[int, int]]):\n\n    super().__init__(env)\n    self.get_current_location = get_current_location\n\n    self.action_space = gym.spaces.Discrete(4)\n\n    self.udlr_action_to_board_action = {\n      0: (0, -1), # up\n      1: (0, +1), # down\n      2: (-1, 0), # left\n      3: (+1, 0), # right\n    }\n\n  def action(self, action: int) -&gt; Tuple[int, int]:\n    current_location = self.get_current_location()\n    return tuple(np.add(current_location, self.udlr_action_to_board_action[action]))\n</code></pre> <p>Note, that you are very welcome to wrap your wrapped environment yet another time, and as many times are you need. A wrapped environment is still just an environment.</p> <p>There are (at least) two additional useful wrapper base classes. The one is ObservationWrapper.</p> <p>For example, you may wish to convert an RGB image into a monocolour image, as you believe it is simpler to learn from monocolour image. You can also build a ObservationWrapper to enrich the observation by introducing a short history to capture motion. Think of a single frame from an ATARI game, say a ball is heading towards you. Can you tell if it is going to the left or to the right? If you are given a few additional frames from the past you may see the direction.</p> <p>Wait, aren't we breaking the Markov Decision Process (MDP) assumption? And what is the MDP assumption to begin with?</p> <p>In order to simplify the world a little, we assume that at any given moment the state of the environment represents everything that we need to know in order to act. What happened before with the environment, our (the agent's) past mistakes, nothing matters.</p> <p>Wanted to use here \"carpe diem\", yet the expression is not appropriate as the future actually is very relevant. You do what is right now, yet of course you need to take into account the future and try to foresee it and what will be the consequences of your current action. Regrets are not helpful, but planning of some sort is the essence of RL. The environment, if indeed an MDP, behaves similarly, subject to randomness, when we return to the same state and take the same action. This is exactly what an agent needs to learn.</p> <p>So, back to the question, did we break the MDP assumption by introducing history? By changing the observation space, we've created a new MDP, for which there are more states. So still an MDP but a more detailed one. A thing that can help or harm the learning process. Yet to experiment.</p> <p>The other useful wrapper base class is RewardWrapper. Why to touch the reward? Maybe a reward of +1 / -1 at the end of the game is correct, but it takes the agent a lot of time to get to a win, and on the way getting no signal for progress is not helpful. We can add indication of good progress by giving some small rewards signals on the way. This will be under reward shaping. A very important concept. Another reason that we may want to change the reward is in order to scale / normalize it for the benefit of our neural network learning optimization.</p>"},{"location":"concepts/action_masking/","title":"Action Masking","text":"<p>When we define an action space for the environment we develop, we set the rules by which an agent can influence its own destiny in the environment. This action space should match the capabilities of the agent and should be relevant for the environment.</p> <p>For example, if the observation space is the cell\u2019s cartesian coordinates (x, y) in a maze where the agent finds itself, then the action space can be, as an example, {up, right, down, left}. It is convenient for the next steps in implementing the agent, that the action space is simple and fixed across all the different observation, which lends itself to a simpler deep neural network architecture. For example one with a fixed-sized output layer of action probabilities.</p> <p>An issue arise when in different situations there are illegal moves. Now we are left to solve the situation in which the agent wants us to communicate to the environment an illegal action. Imagine an illegal Chess move such as moving the agent's Rock to the square where its second Rock is placed. How to address this scenario?</p> <p>A possible solution in such a situation would be to terminate an episode, giving the agent a big negative reward. We hope that the agent learns to avoid illegal moves. A Gym wrapper can be developed that simply ignores an invalid action, bypasses the original environment, and just returns the current observation, the negative reward, and the terminated flag. This may work eventually yet it is somewhat wasteful in the sense that we are in a position to communicate to the agent the rules of the game (environment). This may be somewhat an abuse of AI. An analogy would be to train an agent to multiply integers using supervised learning techniques. We\u2019ll wind up with less than perfect capable agent, after having invested resources on things that should be addressed much simpler. On the other hand the agent may learn to avoid being stuck in a corner, as an example. So keep the option of returning a negative reward and terminating, also in your mind when designing an RL environment / agents.</p> <p>Another option that comes to mind is to ask the agent for an alternative action, or to completely ignore the agent\u2019s wish and take a random (legal) action if the agent fails to provide a valid action. For example use Gym ActionWrapper to achieved above. Care should be taken to communicate the action that was actually taken to the trainer so that the right conclusion shall be drawn from the relevant  interaction of the agent in the environment. The risk with this approach is that we protect our \u201cbaby\u201d agent from facing the real world, which may have consequences of its ability to learn sophisticated behaviors.</p> <p>Eventually we are getting to action masking. The idea is that the environment, or a wrapper of the environment shall communicate to the agent what are the possible moves when the agent is about to take an action. So while the agent is born with the capabilities to act for example {up, right, down, left}, it shall be told, \"but now you should only choose from {right, left}\". In order to achieve this functionality we need both the environment or a wrapper of it to add the legal moves to the observation, and also we need our implementation to be able to take this additional information. For example as an expected optional field in the observation dictionary (as is done in Tianshou), or as a extra parameter for the act / predict / forward call.</p> <p>What happens behind the scenes? One way to address it in the RL library is to combine the mask with the outputs and to make the illegal moves get (almost) zero probability. The property of differential loss is kept and the learning by stochastic decent or by its variant optimization techniques is used. There are additional ways to address that. See for example the following research / survey paper [1]. </p>"},{"location":"concepts/action_masking/#references","title":"References","text":"<p>[1] A Closer Look at Invalid Action Masking in Policy Gradient Algorithms.</p>"}]}
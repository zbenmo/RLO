{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reinforcement Learning Observations","text":"<p>Welcome to my RL related blog.</p> <p>We'll review reinforcement-learning libraries and concepts, and strive to keep track of the state-of-the-art in the RL arena, and to suggest the best engineering practices.</p> <p>We'll talk here about environments and about agents. We'll make observations and we will be rewarded!</p>"},{"location":"reinforcement_learning/","title":"Reinforcement Learning","text":"<p>We want to build software that automates tasks or acts on our behalf in various situations. This is nothing new. We have build software for some time now. We have came up with algorithms and engineering solutions.</p> <p>The future lies apparently with machine learning. We can build parts of our software systems in new ways. By training software from examples. So instead of figuring how to do stuff, we communicate to the software what we want, and the software is built automatically from this description.</p> <p>For example for teaching software to distinguish between cats and dogs in images we provide examples of cats and of dogs and we use supervised learning. This is a machine learning technique with which a model slowly picks up the statistical hints from the labeled examples till it can fulfil the task of predicting correctly the labels on unseen examples, and hence we have achieved a software solution that can carry on from now in telling dogs from cats (and humans can enjoy and supervise while the machines do the hard work).</p> <p>Reinforcement learning is a similar concept. We want software that can play Chess against us, or control a self-driving car, and more. Note that here there are multiple steps. Not just a single decision cat or dog, but rather a series of actions. The software \u201cwins\u201d only if all its moves led to a victory. The car needs to get safely to the destination, following the traffic rules and in a reasonable time. Accelerating, slowing, changing the wheel orientation, all need to contribute for this complex task.</p> <p>To some degree we can employ again supervised learning. In each situation, we can label the desired actions, for example by recording the actions of a human driver. We can then train a model to interpolate desired actions in similar situations. There are many Chess games to learn from, we can try to train a model based on those games. </p> <p>Reinforcement learning is a complementary technique to train models or here we\u2019ll call those agents. Reinforcement learning may be more suitable for those control settings. For example with Chess a move is actually a part of a plan. Learning to repeat a move without gasping what needs to be done later may not work without extensive training set that may not be available, or is too hard to achieve with supervised training. The approach with reinforcement learning is to let the agent learn from interacting with the environment.</p> <p>The environment produces observations and the actions of the agent are communicated to the environment. The environment changes and the agent needs to learn to make smart decisions about its actions. The agent learns this time not from labels but rather by a reward mechanism that we provide to map a transition from one state1 in the environment to another state and its consequences.</p> <p>The agent should learn a policy to map an observation to the best action as to be rewarded as much as possible during the episode. In other words, the agent shall attempt to maximize the cumulative reward to some time horizon. For example the agent shall try to learn to win the Chess game, or to bring the passengers safely to their destination, while avoiding fines and in a reasonable time. There is much to talk about reward and how to design the reward mechanism. If in supervised learning one needs to make sure the labels are correct here with reinforcement learning, the reward mechanism is the equivalent.</p> <p>Note that the environment replaces the training set. By interacting with the environment, the agent can receive infinite feedback. There are some challenges however. We don\u2019t want a car to self-drive in the real world before it is ready. We should have some simulator. The simulator should be very close to the real world and should reflect what happens in the real world. The reward mechanism that is often also associated with the environment should be relevant to learning the desired behavior for the agent.</p> <p>Another very important observation here is that the experience that the agent collects depends on the agent's actions. If a robot keeps going to the left, it will never find out what is there on the right. While in supervised learning we expect the samples in the training set to be of the same distribution (identically independently distributed), the interaction of an agent with an environment are very much dependent on the agent's actions and hence cannot be assumed to be of the same distribution.</p> <ol> <li> <p>A state may mean the same thing as an observation. In many settings the observation is a (partial) derivative of the state, in the sense that the environment's state is more complete and more accurate than the observation that is available to the agent.\u00a0\u21a9</p> </li> </ol>"},{"location":"Gymnasium/gym/","title":"Gym / Gymnasium","text":"<p>When we dive into RL we often discover that the environment is at least as important as the exact algorithm. Not to say that all algorithms are suitable in all situations, but most of us will probably use an existing RL library and hope for the best.</p> <p>The environment on the other hand is where we\u2019ll spend our time to match it to the task we want to achieve. Gym / Gymnasium (Open AI and from now Farama foundation) is currently the de-facto standard for Python environments. A lot of RL Python packages for agents / policies are dependent on Gym. As an example, \u2018stable-baselines3\u2019 (sb3 for short), is dependent on 'torch' but also on \u2018gym\u2019 (and on a few more packages).</p> <p>According to Gym, an environment has the following functionality:</p> <ul> <li> <p>Initial_obs = reset()</p> </li> <li> <p>obs, reward, done, info = step(action)</p> </li> <li> <p>render()</p> </li> </ul> <p>In addition the environment defines the \u2018observation_space\u2019 and the \u2018action_space\u2019. An agent intended to be used with a specific Gym environment, should be adjusted to the relevant \u2018observation_space\u2019 and the relevant \u2018action_space\u2019. Such an agent should provide the functionality to select an action given an observation. The learning setting involves loops where the observations, actions, rewards, and new observations are taken into account to update the policy of the agent based on the relevant RL learning algorithm.</p> <p>Note that the time according to Gym is discrete, in each time step the agent decides on an action based on the current observation and this action is fed into the environment\u2019s \u2018step\u2019 function.</p> <p>The Gym environment is intended for training a single agent. A lot of times this is just what we need. Even if, for example the environment represents a Chess game, and the agent represents the white, then the agent gets a chance to make the first move, then the environment also takes the action for the black. The new observation shall include both moves. Assume for now that we already have some good Chess software that can be used by the environment to play the opponent.</p> <p>If we do want to have multiple agents for a turn-play, or for a simultaneous play with multiple agents, there is an extension of Gym called \u2018PettingZoo\u2019. In PettingZoo, one have an iterator over all the agents and is expected to call 'step' for each.</p> <p>The current state-of-the-art is a bit of a mess. Gym environment\u2019s API was modified so that \u2018step\u2019 now returns instead of \u2018done\u2019, two new outputs: \u2018terminated\u2019, \u2018truncated\u2019. To indicate what exactly happened, whether we reached the \u201cgoal\u201d or we\u2019ve passed the allowed number of steps.</p> <p>Gym \u20180.21.0\u2019 is still with the \u2018done\u2019. After that version we have:</p> <ul> <li>obs, reward, terminated, truncated, info = step(action)</li> </ul> <p>On top of that Open AI donated the code to Farama foundation, and the name was changed to Gymnasium. So you\u2019ll sometimes in Python codes \u2018import gymnasium as gym\u2019. When you start playing with RL, for example with \u2018stable-baselines3\u2019, you most likely want to make sure to use Gym with version \u20180.21.0\u2019 or what sb3 depends on at the time.</p> <p>If you want to create a new environment to represent your challenge, you can consider working with Gymnasium, yet make sure you have a matching RL package / code. You can use Tianshou or you can copy a Python (one file) implementation from CleanRL and do the relevant adjustments.</p> <p>I want to add a side note that I find Gym environment suggested API and in fact the standard, to leave the taste of something missing. I find it horrifying that the existing RL packages are forced to make Gym to be their dependency. A better setting, for me, would be a loosely coupling between the package for the environment and the package for the RL agents / algorithms / training.</p> <p>At the moment Gym / Gymnasium is here to stay, and you must wrap your head around it, that this is the way it works. There are benefits. All RL packages adjust their API to Gym environments so you can quickly try them out on your favorite environment.</p>"},{"location":"Gymnasium/spaces/","title":"Spaces","text":"<p>Gym environment defines observation space and action space. Think of those as the \u201ctypes\u201d of the input and the output for the agent\u2019s decision. The spaces also communicate the range or set of possible values. </p> <p>This makes a lot of sense as when one develops an agent to interact with that environment, the agent needs to be designed to expect observations of, for example images / pixels (ex. an ATARI game), or the output of some sensor, for example representing an angle. The actions of the agent may themselves be a discrete choice from a finite (small) set of possible values, as an example, yet it can also be a collection of values.</p> <pre><code>self.action_space = spaces.Discrete(3)\nself.observation_space = spaces.Box(0, 1, shape=(2,))\n</code></pre> <p>In above example, the action space is 3 possible actions, {0, 1, 2}, standing for example for: {left, right, none}. It is up to the environment to translate the given integer number from the set to the matching action. The observation space in the example above is a vector of two entries, each taking a value from [0, 1]. You can imagine it as a location of a joystick in a 1 by 1 squared box.</p> <p>The reward is a single 'float' number. \u2018truncated\u2019 and \u2018terminated\u2019 (or \u2018done\u2019) are of type 'bool'. There is also a general purpose output from the \u2018step\u2019 function (and also from 'reset'), that is referred to as \u2018info\u2019 and is a dictionary with whatever values the environment wants to communicate to the outside world in addition to the fixed contract given by the observation space and the action space.</p> <p>In the example of a Chess game, we may want to communicate to the agent where are its pieces, where are the other player\u2019s pieces, and in this way unify the cases when the player is the white and when it is actually the black. In Chess we may also want to communicate if specific moves, such as castling, are still valid.</p> <p>What would be the action space for a Chess agent? A na\u00efve approach can be to pick a piece (a square), and move it to another square. A lot of those \u201cactions\u201d are just moving air. A lot of them are trying to move the opponents pieces, or to move a piece of yours to an illegal place. Yet still by this approach of choosing two squares, we may make a simple interface to an RL agent implementation.</p> <p>And so spaces are both the type of the inputs and the outputs, and also the possible values that are accepted. Is it a finite discrete choice, ex. {up, right, down, left}, or two numbers, each of which from [-3.1, 3.8], etc. One can define spaces that are a combination of values of complimentary meaning, values, and types. The combinations are defined, among other options, with Dict spaces.</p> <p>The environment should define it observation space and its action space based on its needs, however, once defined, we need to find RL algorithm implementation that supports the required observation and action spaces. </p> <p>For example, if the environment wish to provide images as observations, we may need an RL implementation that have maybe a CNN. If the action space is discrete we may have the output to be a fixed size layer with softmax activation. Dict observations may require some software construct that shall combine those in some clever way to the inputs of a NN for example.</p> <p>A nice feature of spaces is that you can sample from them. Which means that you can play with an environment, even before having any agent. For example (taken from Gymnasium documentation):</p> <pre><code>import gymnasium as gym\nenv = gym.make(\"LunarLander-v2\", render_mode=\"human\")\nobservation, info = env.reset()\n\nfor _ in range(1000):\n    action = env.action_space.sample()  # agent policy that uses the observation and info\n    observation, reward, terminated, truncated, info = env.step(action)\n\n    if terminated or truncated:\n        observation, info = env.reset()\n\nenv.close()\n</code></pre> <p>Note that sampling from the observation space may return \"images\" that should never actually be returned from the environment (ex. complete random noise). That is because the observation space is wider than the actual distribution of values the agent will actually see. The action space may also be wider than the legal actions. If you stample on such an issue, you may want to read about action masking.</p> <p>Another interesting item to make our lives a bit more interesting, is for example a game like Bridge where you have multiple stages. In Contract Bridge we have the bidding, and the rest of the game. The observation space as well as the action space of an environment are fixed. It is up to you to come up with a setting that can handle all the stages. For example by having an agent for stage, each interacting with one environment dedicated for that stage, or by concatenating somehow the actions, and adding in the observation some hint in what stage we are. Or also as mentioned above by using action masking.</p> <p>As far as I know, most environments do not allow for setting their state explicitly, but rather they either start from the same point (ex. a Chess game), or start from a random state (ex. a Contract Bridge cards shuffle). If relevant, you may hack your environment to start from a specific state, this would be just software engineering.</p>"},{"location":"concepts/action_masking/","title":"Action Masking","text":"<p>When we define an action space for the environment we develop, we set the rules by which an agent can influence its own destiny in the environment. This action space should match the capabilities of the agent and should be relevant for the environment.</p> <p>For example, if the observation space is the cell\u2019s cartesian coordinates (x, y) in a maze where the agent finds itself, then the action space can be, as an example, {up, right, down, left}. It is convenient for the next steps in implementing the agent, that the action space is simple and fixed across all the different observation, which lends itself to a simpler deep neural network architecture. For example one with a fixed-sized output layer of action probabilities.</p> <p>An issue arise when in different situations there are illegal moves. Now we are left to solve the situation in which the agent wants us to communicate to the environment an illegal action. Imagine an illegal Chess move such as moving the agent's Rock to the square where its second Rock is placed. How to address this scenario?</p> <p>A possible solution in such a situation would be to terminate an episode, giving the agent a big negative reward. We hope that the agent learns to avoid illegal moves. A Gym wrapper can be developed that simply ignores an invalid action, bypasses the original environment, and just returns the current observation, the negative reward, and the terminated flag. This may work eventually yet it is somewhat wasteful in the sense that we are in a position to communicate to the agent the rules of the game (environment). This may be somewhat an abuse of AI. An analogy would be to train an agent to multiply integers using supervised learning techniques. We\u2019ll wind up with less than perfect capable agent, after having invested resources on things that should be addressed much simpler. On the other hand the agent may learn to avoid being stuck in a corner, as an example. So keep the option of returning a negative reward and terminating, also in your mind when designing an RL environment / agents.</p> <p>Another option that comes to mind is to ask the agent for an alternative action, or to completely ignore the agent\u2019s wish and take a random (legal) action if the agent fails to provide a valid action. For example use Gym ActionWrapper to achieved above. Care should be taken to communicate the action that was actually taken to the trainer so that the right conclusion shall be drawn from the relevant  interaction of the agent in the environment. The risk with this approach is that we protect our \u201cbaby\u201d agent from facing the real world, which may have consequences of its ability to learn sophisticated behaviors.</p> <p>Eventually we are getting to action masking. The idea is that the environment, or a wrapper of the environment shall communicate to the agent what are the possible moves when the agent is about to take an action. So while the agent is born with the capabilities to act for example {up, right, down, left}, it shall be told, \"but now you should only choose from {right, left}\". In order to achieve this functionality we need both the environment or a wrapper of it to add the legal moves to the observation, and also we need our implementation to be able to take this additional information. For example as an expected optional field in the observation dictionary (as is done in Tianshou), or as a extra parameter for the act / predict / forward call.</p> <p>What happens behind the scenes? One way to address it in the RL library is to combine the mask with the outputs and to make the illegal moves get (almost) zero probability. The property of differential loss is kept and the learning by stochastic decent or by its variant optimization techniques is used. There are additional ways to address that. See for example the following research / survey paper [1]. </p>"},{"location":"concepts/action_masking/#references","title":"References","text":"<p>[1] A Closer Look at Invalid Action Masking in Policy Gradient Algorithms.</p>"}]}
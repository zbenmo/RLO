{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reinforcement Learning Observations","text":"<p>Welcome to my RL related blog.</p> <p>We'll review reinforcement-learning libraries and concepts, and strive to keep track of the state-of-the-art in the RL arena, and to suggest the best engineering practices.</p> <p>We'll talk here about environments and about agents. We'll make observations and we will be rewarded!</p>"},{"location":"reinforcement_learning/","title":"Reinforcement Learning","text":"<p>We want to build software that automates tasks or acts on our behalf in various situations. This is nothing new. We have build software for some time now. We have came up with algorithms and engineering solutions.</p> <p>The future lies apparently with machine learning. We can build parts of our software systems in new ways. By training software from examples. So instead of figuring how to do stuff, we communicate to the software what we want, and the software is built automatically from this description.</p> <p>For example for teaching software to distinguish between cats and dogs in images we provide examples of cats and of dogs and we use supervised learning. This is a machine learning technique with which a model slowly picks up the statistical hints from the labeled examples till it can fulfil the task of predicting correctly the labels on unseen examples, and hence we have achieved a software solution that can carry on from now in telling dogs from cats (and humans can enjoy and supervise while the machines do the hard work).</p> <p>Reinforcement learning is a similar concept. We want software that can play Chess against us, or control a self-driving car, and more. Note that here there are multiple steps. Not just a single decision cat or dog, but rather a series of actions. The software \u201cwins\u201d only if all its moves led to a victory. The car needs to get safely to the destination, following the traffic rules and in a reasonable time. Accelerating, slowing, changing the wheel orientation, all need to contribute for this complex task.</p> <p>To some degree we can employ again supervised learning. In each situation, we can label the desired actions, for example by recording the actions of a human driver. We can then train a model to interpolate desired actions in similar situations. There are many Chess games to learn from, we can try to train a model based on those games. </p> <p>Reinforcement learning is a complementary technique to train models or here we\u2019ll call those agents. Reinforcement learning may be more suitable for those control settings. For example with Chess a move is actually a part of a plan. Learning to repeat a move without gasping what needs to be done later may not work without extensive training set that may not be available, or is too hard to achieve with supervised training. The approach with reinforcement learning is to let the agent learn from interacting with the environment.</p> <p>The environment produces observations and the actions of the agent are communicated to the environment. The environment changes and the agent needs to learn to make smart decisions about its actions. The agent learns this time not from labels but rather by a reward mechanism that we provide to map a transition from one state1 in the environment to another state and its consequences.</p> <p>The agent should learn a policy to map an observation to the best action as to be rewarded as much as possible during the episode. In other words, the agent shall attempt to maximize the cumulative reward to some time horizon. For example the agent shall try to learn to win the Chess game, or to bring the passengers safely to their destination, while avoiding fines and in a reasonable time. There is much to talk about reward and how to design the reward mechanism. If in supervised learning one needs to make sure the labels are correct here with reinforcement learning, the reward mechanism is the equivalent.</p> <p>Note that the environment replaces the training set. By interacting with the environment, the agent can receive infinite feedback. There are some challenges however. We don\u2019t want a car to self-drive in the real world before it is ready. We should have some simulator. The simulator should be very close to the real world and should reflect what happens in the real world. The reward mechanism that is often also associated with the environment should be relevant to learning the desired behavior for the agent.</p> <p>Another very important observation here is that the experience that the agent collects depends on the agent's actions. If a robot keeps going to the left, it will never find out what is there on the right. While in supervised learning we expect the samples in the training set to be of the same distribution (identically independently distributed), the interaction of an agent with an environment are very much dependent on the agent's actions and hence cannot be assumed to be of the same distribution.</p> <ol> <li> <p>A state may mean the same thing as an observation. In many settings the observation is a (partial) derivative of the state, in the sense that the environment's state is more complete and more accurate than the observation that is available to the agent.\u00a0\u21a9</p> </li> </ol>"},{"location":"Gymnasium/gym/","title":"Gym / Gymnasium","text":"<p>When we dive into RL we often discover that the environment is at least important as the exact algorithm. Not to say that all algorithms are suitable in all situations, but most of us will probably use an existing RL library and hope for the best.</p> <p>The environment on the other hand is where we\u2019ll spend our time to match it to the task we want to achieve. Gym / Gymnasium (Open AI / Farama) is currently the de-facto standard for Python environments. A lot of RL Python packages for agents / policies are dependent on Gym. For example \u2018stable-baselines3\u2019 (sb3 for short) is dependent on \u2018gym\u2019, among other packages such as \u2018torch\u2019.</p> <p>According to Gym, an environment has the following functionality:</p> <ul> <li> <p>Initial_obs = reset()</p> </li> <li> <p>obs, reward, done, info = step(action)</p> </li> <li> <p>render()</p> </li> </ul> <p>In addition the environment defines the \u2018observation_space\u2019 and the \u2018action_space\u2019. An agent intended to be used with a specific Gym environment, should be adjusted to the relevant \u2018observation_space\u2019 and the relevant \u2018action_space\u2019. Such and agent should provide the functionality to select and action given an observation. The learning setting involves loops where the observations, actions, rewards, and new observations are taken into account to update the policy of the agent based on the relevant RL learning algorithm.</p> <p>Note that the time according to Gym is discrete, in each time step the agent decides on an action based on the current observation and this action is fed into the environment\u2019s \u2018step\u2019 function.</p> <p>The Gym environment is intended for training a single agent. A lot of times this is just what we need. Even if, for example the environment represents Chess game, and the agent represents the white, then the agent gets a chance to make the first move, then the environment also takes the action for the black. The new observation shall include both moves. Assume for now that we already have some good Chess software that can be used by the environment to play the opponent.</p> <p>If we do want to have multiple agents for a turn-play, or for a simultaneous play with multiple agents, there is an extension of Gym called \u2018PettingZoo\u2019. In PettingZoo, one have an iterator over all the agents and is expected to call step for each.</p> <p>The current state-of-the-art is a bit of a mess. Gym environment\u2019s API was modified so that \u2018step\u2019 now returns instead of \u2018done\u2019, two new outputs: \u2018terminated\u2019, \u2018truncated\u2019. To indicate what exactly happened, whether we reached the \u201cgoal\u201d or we\u2019ve passed the allowed number of steps. Gym \u20180.21.0\u2019 is still with the \u2018done\u2019.</p> <p>After that version we have:</p> <ul> <li>obs, reward, terminated, truncated, info = step(action)</li> </ul> <p>On top of that Open AI donated the code to Farama foundation, and the name was changed to Gymnasium. So you\u2019ll sometimes in Python codes \u2018import gymnasium as gym\u2019. When you start playing with RL, for example with \u2018stable-baselines3\u2019, you most likely want to make sure to use Gym with version \u20180.21.0\u2019 or what sb3 depends on at the time.</p> <p>If you want to create a new environment to represent your challenge, you can consider working with Gymnasium, yet make sure you have a matching RL package / code. You can use Tianshou or you can copy a Python (one file) implementation from CleanRL and do the relevant adjustments.</p> <p>I want to add a side note that I find Gym environment suggested API and in fact the standard, to leave the taste of something missing. I find it horrifying that the existing RL packages are forced to make Gym to be their dependency. A better setting, for me, would be a loosely coupling between the package for the environment and the package for the RL agents / algorithms / training.</p> <p>At the moment Gym / Gymnasium is here to stay, and you must wrap your head around it, what this is the way it works. There are benefits. All RL packages adjust their API to Gym environments so you can quickly try them out on your favorite environment.</p>"},{"location":"concepts/action_masking/","title":"Action Masking","text":"<p>When we define an action space for the environment we develop, we set the rules by which an agent can influence its own destiny in the environment. This action space should match the capabilities of the agent and should be relevant for the environment.</p> <p>For example, if the observation space is the cell\u2019s cartesian coordinates (x, y) in a maze where the agent finds itself, then the action space can be, as an example, {up, right, down, left}. It is convenient for the next steps in implementing the agent, that the action space is simple and fixed across all the different observation, which lends itself to a simpler deep neural network architecture. For example one with a fixed-sized output layer of action probabilities.</p> <p>An issue arise when in different situations there are illegal moves. Now we are left to solve the situation in which the agent wants us to communicate to the environment an illegal action. Imagine an illegal Chess move such as moving the agent's Rock to the square where its second Rock is placed. How to address this scenario?</p> <p>A possible solution in such a situation would be to terminate an episode, giving the agent a big negative reward. We hope that the agent learns to avoid illegal moves. A Gym wrapper can be developed that simply ignores an invalid action, bypasses the original environment, and just returns the current observation, the negative reward, and the terminated flag. This may work eventually yet it is somewhat wasteful in the sense that we are in a position to communicate to the agent the rules of the game (environment). This may be somewhat an abuse of AI. An analogy would be to train an agent to multiply integers using supervised learning techniques. We\u2019ll wind up with less than perfect capable agent, after having invested resources on things that should be addressed much simpler. On the other hand the agent may learn to avoid being stuck in a corner, as an example. So keep the option of returning a negative reward and terminating, also in your mind when designing an RL environment / agents.</p> <p>Another option that comes to mind is to ask the agent for an alternative action, or to completely ignore the agent\u2019s wish and take a random (legal) action if the agent fails to provide a valid action. For example use Gym ActionWrapper to achieved above. Care should be taken to communicate the action that was actually taken to the trainer so that the right conclusion shall be drawn from the relevant  interaction of the agent in the environment. The risk with this approach is that we protect our \u201cbaby\u201d agent from facing the real world, which may have consequences of its ability to learn sophisticated behaviors.</p> <p>Eventually we are getting to action masking. The idea is that the environment, or a wrapper of the environment shall communicate to the agent what are the possible moves when the agent is about to take an action. So while the agent is born with the capabilities to act for example {up, right, down, left}, it shall be told, \"but now you should only choose from {right, left}\". In order to achieve this functionality we need both the environment or a wrapper of it to add the legal moves to the observation, and also we need our implementation to be able to take this additional information. For example as an expected optional field in the observation dictionary (as is done in Tianshou), or as a extra parameter for the act / predict / forward call.</p> <p>What happens behind the scenes? One way to address it in the RL library is to combine the mask with the outputs and to make the illegal moves get (almost) zero probability. The property of differential loss is kept and the learning by stochastic decent or by its variant optimization techniques is used. There are additional ways to address that. See for example the following research / survey paper [1]. </p>"},{"location":"concepts/action_masking/#references","title":"References","text":"<p>[1] A Closer Look at Invalid Action Masking in Policy Gradient Algorithms.</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reinforcement Learning Observations","text":"<p>Welcome to my RL related blog.</p> <p>We'll review reinforcement-learning libraries and concepts, and strive to keep track of the state-of-the-art in the RL arena, and to suggest the best engineering practices.</p> <p>We'll talk here about environments and about agents. We'll make observations and we will be rewarded!</p>"},{"location":"reinforcement_learning/","title":"Reinforcement Learning","text":"<p>We want to build software that automates tasks or acts on our behalf in various situations. This is nothing new. We have build software for some time now. We have came up with algorithms and engineering solutions.</p> <p>The future lies apparently with machine learning. We can build parts of our software systems in new ways; by training software from examples. So instead of figuring how to do stuff, we communicate to the software what we want, and the software is built automatically from this \"description\".</p> <p>To teach software to distinguish between cats and dogs in images, we provide examples of cats and examples of dogs and we use supervised learning. Supervised learning is a machine learning technique in which a model picks up statistical hints from labeled examples, desirably till it can fulfil the task of predicting correctly the labels on unseen examples. Hence we have achieved a software solution that can carry on from now, in telling dogs from cats (and humans can enjoy and supervise, while the machines do the hard work).</p> <p>Reinforcement learning is a similar concept. We want software that can play Chess against us, or control a self-driving car, and more. Note that here there are multiple steps. Not just a single decision; cat or dog, but rather a series of actions. The software \u201cwins\u201d only if all its moves led to a victory. The car needs to get safely to the destination, following the traffic rules and in a reasonable time. Accelerating, slowing, changing the wheel orientation, all need to contribute for this complex task.</p> <p>To some degree we can employ again supervised learning. In each situation (state), we can label the desired action, for example by recording the actions of a human driver. We can then train a model to imitate the human by taking similar actions in similar situations. There are many recorded Chess games to learn from, we can try to train a model based on those games. Can we do even better than our role model? Can we watch someone else, learn both from their successes and also from their mistakes? Can we combine the tricks that we learn from multiple teachers? Can we try ourselves and gain some additional experience?</p> <p>Reinforcement learning is a complementary technique to train models or here we\u2019ll call those agents. Reinforcement learning may be more suitable for those control settings. For example with Chess, a move is actually a part of a plan. Learning to repeat a move without gasping what needs to be done later may not work without extensive training set that may not be available, or is too hard to achieve with supervised training. The approach with reinforcement learning is to let the agent learn from interacting with the environment.</p> <p>The environment produces observations and the actions of the agent are communicated to the environment. The state of the environment changes and the agent needs to learn to make smart decisions about its actions. The agent learns this time not from labels but rather by a reward mechanism that we provide to map a transition from one state1 in the environment to another state and its consequences.</p> <p>I'm bringing below an image taken from Wikipedia. In this image, we see yet another component in the setting. This is the interpreter.</p> Reinforecement Learning Conceptual Model - Wikipedia <p>The interpreter is included in the conceptual model as we can argue that the environment is ignorant to the agent's needs and what rewards the agent. The agent may actually observe only a partial or a distorted view of the current environment's state. One may suggest that the interpreter is part of the agent, say its eyes and inner feelings. In practice the interpreter is usually included in the environment and hence the environment can be considered as the agent's private view of the world; what happens to the agent while the agent is operating in what the agent believes to be its world. In a Chess game, one can imagine that there are two environments. One environment for one player and anoter environment for the other player, where the two environments are being synchronized somehow behind the scenes. The environment can be shared among multiple agents in a setting where they need to compete and/or coordinate. The reward may be unique for each of the agents, or shared. The same goes for the observation.</p> <p>The agent should learn a policy to map an observation to the best action as to be rewarded as much as possible during the episode. In other words, the agent shall attempt to maximize the cumulative reward to some time horizon. For example the agent shall try to learn to win the Chess game, or to bring the passengers safely to their destination, while avoiding fines and in a reasonable time. There is much to talk about reward and how to design the reward mechanism. If in supervised learning one needs to make sure the labels are correct, here with reinforcement learning, the reward mechanism is the equivalent.</p> <p>Note that the environment replaces the training set. By interacting with the environment, the agent can receive infinite feedback. There are some challenges however. We don\u2019t want a car to self-drive in the real world before it is ready. We should have some simulator. The simulator should be very close to the real world and should reflect what happens in the real world. The reward mechanism, that is often also associated with the environment, should be relevant to learning the desired behavior for the agent.</p> <p>Another very important observation here is that the experience that the agent collects depends on the agent's actions. If a robot keeps going to the left, it will never find out what is there on the right. While in supervised learning we expect the samples in the training set to be of the same distribution (identically independently distributed), the interaction of an agent with an environment are very much dependent on the agent's actions and hence cannot be assumed to be of the same distribution.</p> <ol> <li> <p>A state may mean the same thing as an observation. In many settings the observation is a (partial) derivative of the state, in the sense that the environment's state is more complete and more accurate than the observation that is available to the agent.\u00a0\u21a9</p> </li> </ol>"},{"location":"Gymnasium/creating_a_new_environment/","title":"Creating a new Gymnasium environment","text":"<p>Developing a new Gymnasium/Gym environment involves subclassing gym.Env (potentially you declared <code>import gymnasium as gym</code>). You need to implement the following:</p> <ul> <li>reset</li> <li>step</li> <li>render</li> </ul> <p>In addition, your environment should have the following two attributes / member variables: observation_space and observation_space. For example in the initialization of your environment you can have (copied from Gymnasium documentation):</p> <pre><code>    ...\nself.observation_space = spaces.Dict(\n{\n\"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n\"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n}\n)\n# We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\nself.action_space = spaces.Discrete(4)\n...\n</code></pre> <p>'reset' should bring the environment to some initial state. It might involve randomness if relevant, or just to the starting position of, for example Chess game. 'reset' returns the initial observation.</p> <p>For Gymnasium, the signature of 'reset' looks as follows:</p> <pre><code>def reset(self, seed=None, options=None):\n...\n</code></pre> <p>It is a good idea to pass the 'seed' to 'super' even if no randomness is expected, to avoid environment checks failure.</p> <pre><code>def reset(self, seed=None, options=None):\n# We need the following line to seed self.np_random\nsuper().reset(seed=seed)\n...\n</code></pre> <p>As mentioned above 'reset' should return the first observation and I'm adding here, that also a \"free form\" info. For example:</p> <pre><code>def reset(self, seed=None, options=None):\n...\nreturn self._get_obs(), self._get_info()\n</code></pre> <p>It is a good idea to \"wrap\" the logic of constructing the observation from the state in a function as you'll need it also in 'step'. The information output can be just an empty dict, <code>{}</code>, if you wish.</p> <p>'step' receives and action and should return a tuple with the following: the next observation, the reward, was the environment terminated, was the environment truncated, and again some info.</p> <p>So for example:</p> <pre><code>def step(self, action):\n...\nreward = 1\ndone = False\ninfo = {'mask': [True, False, False, True]} # an interested agent can use this mask to avoid illegal moves\nreturn self._get_obs(), reward, done, False, info\n</code></pre> <p>'render' should for example nicely print the environment's state to the console.</p> <p>My suggestion would be to develop your environment without thinking about RL nor about Gym. Let's call this environment the \"native\" environment. Try to make your \"native\" environment passive in the sense, that in order to use it, one needs to call the methods of the environment from a loop. I've implemented an environment I'm calling \"Collect Coins\". See in qwertyenv.</p> <p>My environment is a turns game, two players, similar to Chess. The aim of the game is to collect more coins from the board than the other player. I've started by creating the \"game\" in a separate Python file. This is not a Gym environment, yet it enabled me to \"play\" in turn the white and then the black and so on till the game is over. I've added a check that you are playing the right player when you call the \"play\" function. Added also a verification that the play is legal. I have no reward mechanism other than allowing the user of the \"game\" to see the board, the current counts of coins, and if the game is done, who won.</p> <p>Now, to make it into an environment, I had to do the following:</p> <ul> <li>Keep an instance of the \"game\" in my environment. In 'reset' I could opt to create a new instance, or to call the equivalent on the self.game instance. I created a new instance of the \"game\" on each 'reset'.</li> <li>Find the Gym spaces that can be mapped to my \"game\" board and status, and the possible moves. For example, in my game, the board is a boolean 8x8 matrix where you still have coins, and in addition you have the location of the player and the location of the other player. Those three pieces of observation went into gym.spaces.Dict. Coming to think of it, an agent may wish to see the count of coins. I did not made this freely available for the agent in a straight manner in this version.</li> <li>The action space, I've decided to have as the target square. The reason is that in my game you can have various Chess pieces (this is determined in the initialization of the game). I have at the moment knights and rocks. The rocks are actually a limited version, where they can be moved only to a neighbour square. Maybe I'll add in the future also a \"real\" rock. By having an action space of the target square, I've avoided having two different action spaces for each. I'm just telling what I've done, don't take from that that it was a smart decision.  </li> <li>Develop a reward mechanism. In my first iteration I went for +1 for a win, 0 for draw, and -1 for a loss. </li> <li>Now I had an engineering challenge. Let's say I let the external agent act as the first player (white). But where will the other player come from? This had to be implemented in the environment. So towards the end of the 'step' method, I've took a move for the other player. In the 'reset' this logic was also used if the external agent is the black and needs to move second.</li> <li>The 'render' functionality was implemented in the \"native\" game environment.</li> </ul> <p>If you build a Python package, it is a good practice to 'register' your Gym environment, so that one can later find it for example with <code>env =  gym.make('qwertyenv/CollectCoins-v0', pieces=['rock', 'rock'])</code></p> <p>A registration is done usually in the __init__.py of the package:</p> <pre><code>from gym.envs.registration import register\n...\nregister(\nid='qwertyenv/CollectCoins-v0',\nentry_point='qwertyenv.collect_coins:CollectCoinsEnv',\nmax_episode_steps=300\n)\n...\n</code></pre> <p>Here are some lessons I learned.</p> <p>Make sure to debug the \"native\" environment (asserts, pytest). </p> <p>For your Gym environment, write as a minimum, a pytest to call:</p> <pre><code>from gymnasium.utils.env_checker import check_env\n..\n...\ndef test_my_env():\n..\ncheck_env(env)\n</code></pre> <p>The observation_space and the action_space, have in addition to \"sample\", also a very useful functionality for debugging. One can check if the space \"contains\" a specific value that he/she wants to return as an observation, or pass as an action. When the relevant space does not contain the value, this must be first solved by either changing the space, or realizing that the action or observation indicate a potential bug.</p> <p>Above enabled me to test with SB3 (I wrote it for Gym '0.21.0'). I will tell more about my experiments when discussing Gym wrappers and PettingZoo (I will write about it soon......)</p>"},{"location":"Gymnasium/gym/","title":"Gym/Gymnasium","text":"<p>When we dive into RL we often discover that the environment is at least as important as the exact algorithm. Not to say that all algorithms are suitable in all situations, but most of us will probably use an existing RL library and hope for the best.</p> <p>The environment on the other hand is where we\u2019ll spend our time to match it to the task we want to achieve. Gym/Gymnasium (Open AI and from now Farama foundation) is currently the de-facto standard for Python environments. A lot of RL Python packages for agents / policies are dependent on Gym. As an example, \u2018stable-baselines3\u2019 (sb3 for short), is dependent on 'torch' but also on \u2018gym\u2019 (and on a few more packages).</p> <p>According to Gym, an environment has the following functionality:</p> <ul> <li> <p>Initial_obs = reset()</p> </li> <li> <p>obs, reward, done, info = step(action)</p> </li> <li> <p>render()</p> </li> </ul> <p>In addition the environment defines the \u2018observation_space\u2019 and the \u2018action_space\u2019. An agent intended to be used with a specific Gym environment, should be adjusted to the relevant \u2018observation_space\u2019 and the relevant \u2018action_space\u2019. Such an agent should provide the functionality to select an action given an observation. The learning setting involves loops where the observations, actions, rewards, and new observations are taken into account to update the policy of the agent based on the relevant RL learning algorithm.</p> <p>Note that the time according to Gym is discrete, in each time step the agent decides on an action based on the current observation and this action is fed into the environment\u2019s \u2018step\u2019 function.</p> <p>The Gym environment is intended for training a single agent. A lot of times this is just what we need. Even if, for example the environment represents a Chess game, and the agent represents the white, then the agent gets a chance to make the first move, then the environment also takes the action for the black. The new observation shall include both moves. Assume for now that we already have some good Chess software that can be used by the environment to play the opponent.</p> <p>If we do want to have multiple agents for a turn-play, or for a simultaneous play with multiple agents, there is an extension of Gym called \u2018PettingZoo\u2019. In PettingZoo, one have an iterator over all the agents and is expected to call 'step' for each.</p> <p>The current state-of-the-art is a bit of a mess. Gym environment\u2019s API was modified so that \u2018step\u2019 now returns instead of \u2018done\u2019, two new outputs: \u2018terminated\u2019, \u2018truncated\u2019. To indicate what exactly happened, whether we reached the \u201cgoal\u201d or we\u2019ve passed the allowed number of steps.</p> <p>Gym \u20180.21.0\u2019 is still with the \u2018done\u2019. After that version we have:</p> <ul> <li>obs, reward, terminated, truncated, info = step(action)</li> </ul> <p>On top of that Open AI donated the code to Farama foundation, and the name was changed to Gymnasium. So you\u2019ll sometimes in Python codes \u2018import gymnasium as gym\u2019. When you start playing with RL, for example with \u2018stable-baselines3\u2019, you most likely want to make sure to use Gym with version \u20180.21.0\u2019 or what sb3 depends on at the time.</p> <p>If you want to create a new environment to represent your challenge, you can consider working with Gymnasium, yet make sure you have a matching RL package / code. You can use Tianshou or you can copy a Python (one file) implementation from CleanRL and do the relevant adjustments.</p> <p>I want to add a side note that I find Gym environment suggested API and in fact the standard, to leave the taste of something missing. I find it horrifying that the existing RL packages are forced to make Gym to be their dependency. A better setting, for me, would be a loosely coupling between the package for the environment and the package for the RL agents / algorithms / training.</p> <p>At the moment Gym/Gymnasium is here to stay, and you must wrap your head around it, that this is the way it works. There are benefits. All RL packages adjust their API to Gym environments so you can quickly try them out on your favorite environment.</p>"},{"location":"Gymnasium/spaces/","title":"Spaces","text":"<p>Gym environment defines observation space and action space. Think of those as the \u201ctypes\u201d of the input and the output for the agent\u2019s decision. The spaces also communicate the range or set of possible values. </p> <p>This makes a lot of sense as when one develops an agent to interact with that environment, the agent needs to be designed to expect observations of, for example images / pixels (ex. an ATARI game), or the output of some sensor, for example representing an angle. The actions of the agent may themselves be a discrete choice from a finite (small) set of possible values, as an example, yet it can also be a collection of values.</p> <pre><code>self.action_space = spaces.Discrete(3)\nself.observation_space = spaces.Box(0, 1, shape=(2,))\n</code></pre> <p>In above example, the action space is 3 possible actions, {0, 1, 2}, standing for example for: {left, right, none}. It is up to the environment to translate the given integer number from the set to the matching action. The observation space in the example above is a vector of two entries, each taking a value from [0, 1]. You can imagine it as a location of a joystick in a 1 by 1 squared box.</p> <p>The reward is a single 'float' number. \u2018truncated\u2019 and \u2018terminated\u2019 (or \u2018done\u2019) are of type 'bool'. There is also a general purpose output from the \u2018step\u2019 function (and also from 'reset'), that is referred to as \u2018info\u2019 and is a dictionary with whatever values the environment wants to communicate to the outside world in addition to the fixed contract given by the observation space and the action space.</p> <p>In the example of a Chess game, we may want to communicate to the agent where are its pieces, where are the other player\u2019s pieces, and in this way unify the cases when the player is the white and when it is actually the black. In Chess we may also want to communicate if specific moves, such as castling, are still valid.</p> <p>What would be the action space for a Chess agent? A na\u00efve approach can be to pick a piece (a square), and move it to another square. A lot of those \u201cactions\u201d are just moving air. A lot of them are trying to move the opponents pieces, or to move a piece of yours to an illegal place. Yet still by this approach of choosing two squares, we may make a simple interface to an RL agent implementation.</p> <p>And so spaces are both the type of the inputs and the outputs, and also the possible values that are accepted. Is it a finite discrete choice, ex. {up, right, down, left}, or two numbers, each of which from [-3.1, 3.8], etc. One can define spaces that are a combination of values of complimentary meaning, values, and types. The combinations are defined, among other options, with Dict spaces.</p> <p>The environment should define its observation space and its action space based on its needs, however, once defined, we need to find RL algorithm implementation that supports the required observation and action spaces. </p> <p>For example, if the environment wish to provide images as observations, we may need an RL implementation that have maybe a CNN. If the action space is discrete we may have the output to be a fixed size layer with softmax activation. Dict observations may require some software construct that shall combine those in some clever way to the inputs of a NN for example.</p> <p>A nice feature of spaces is that you can sample from them. Which means that you can play with an environment, even before having any agent. For example (taken from Gymnasium documentation):</p> <pre><code>import gymnasium as gym\nenv = gym.make(\"LunarLander-v2\", render_mode=\"human\")\nobservation, info = env.reset()\nfor _ in range(1000):\naction = env.action_space.sample()  # agent policy that uses the observation and info\nobservation, reward, terminated, truncated, info = env.step(action)\nif terminated or truncated:\nobservation, info = env.reset()\nenv.close()\n</code></pre> <p>Note that sampling from the observation space may return \"images\" that should never actually be returned from the environment (ex. complete random noise). That is because the observation space is wider than the actual distribution of values the agent will actually see. The action space may also be wider than the legal actions. If you stample on such an issue, you may want to read about action masking.</p> <p>Another item to make our lives even a bit more interesting, is for example a game like Contract Bridge where you have multiple stages. In Contract Bridge we have the bidding, and the rest of the game. The observation space as well as the action space of an environment are fixed. It is up to you to come up with a setting that can handle all the stages. For example by having an agent for stage, each interacting with one environment dedicated for that stage, or by concatenating somehow the action apaces, and adding in the observation some hint in which stage we are. Or also as mentioned above by using action masking.</p> <p>As far as I know, most environments do not allow for setting their state explicitly, but rather they either start from the same point (ex. a Chess game), or start from a random state (ex. a Contract Bridge cards shuffle). If relevant, you may hack your environment to start from a specific state, this would be just software engineering.</p>"},{"location":"Gymnasium/wrappers/","title":"Wrappers","text":"<p>A wrapper in Gym/Gymnasium is just an environment that delegates its calls to another environment, potentially doing first some adjustments.</p> <p>Why is it a good idea to introduce wrappers and when will we want to use wrappers?</p> <p>The agents developed against Gym environments in mind, expect a specific interface. That is in particular the 'step' function. One is expected to pass as an argument to the 'step' function an action from the action space and one expects to receive in return an observation (the next observation) from the observation space. The reward that is returned from the 'step' function determines also if the agent is able to learn the desired behaviour, and is not less important from the observation and the learning algorithm itself.</p> <p>What do you do when you want your agent to think of the problem a little different. What do you do if you want the action space to be a bit different?</p> <p>I'll give an example. The environment expects a target destination on an '8x8' board game. You train an agent with your preferred RL library, and a suitable method, yet the agent seems to learn very slow. Let's assume that the agent avoids illegal moves, for example, by taking into account action masking. Still the \"learning\" is slow. You brainstorm with friends and they point that your (agent) piece can only move {up, left, down, right}. You hypnotise that if the action space is changed to those four possible actions, the agent shall learn faster. To try, you quickly wrap the original environment, with an environment that declares its \u2018action_space\u2019 with above four (discrete) directions, and translates this action into the target square. You can now train an agent with the \"new\" environment and see if the learning is faster.</p>"},{"location":"Gymnasium/wrappers/#actionwrapper","title":"ActionWrapper","text":"<p>While you can create a new environment to wrap the old environment, or also to create your custom wrapper based on the class gym.Wrapper, in above case you can actually base your wrapper on the class gym.ActionWrapper. If you base your wrapper on ActionWrapper, you only need to have the new action_space available, and to implement a function called action, that takes an action from the new action space (the one of the wrapper), and returns the matching action of the wrapped environment. For example, something I did in qwertyenv.</p> <pre><code>import gym\nfrom typing import Tuple, Callable\nimport numpy as np\nclass UpDownLeftRight(gym.ActionWrapper):\n\"\"\"\n  A gym environment wrapper to enable U/D/L/R action space when the wrapped environment\n  actually expects a destination in cartesian coordinates.\n  \"\"\"\ndef __init__(self, env: gym.Env,\nget_current_location: Callable[[], Tuple[int, int]]):\nsuper().__init__(env)\nself.get_current_location = get_current_location\nself.action_space = gym.spaces.Discrete(4)\nself.udlr_action_to_board_action = {\n0: (0, -1), # up\n1: (0, +1), # down\n2: (-1, 0), # left\n3: (+1, 0), # right\n}\ndef action(self, action: int) -&gt; Tuple[int, int]:\ncurrent_location = self.get_current_location()\nreturn tuple(np.add(current_location, self.udlr_action_to_board_action[action]))\n</code></pre> <p>Note, that you are very welcome to wrap your wrapped environment yet another time, and as many times are you need. A wrapped environment is still just an environment.</p>"},{"location":"Gymnasium/wrappers/#observationwrapper","title":"ObservationWrapper","text":"<p>There are (at least) two additional useful wrapper base classes. The one is ObservationWrapper. For example, you may wish to convert an RGB image into a monocolour image, as you believe it is simpler to learn from monocolour image.</p> <p>You can also build a ObservationWrapper to enrich the observation by introducing a short history to capture motion. Think of a single frame from an ATARI game, say a ball is heading towards you. Can you tell if it is going to the left or to the right? If you are given a few additional frames from the past you may see the direction (frame stacking).</p> ATARI Breakout <p>Wait, aren't we breaking the Markov property assumption? And what is the Markov property assumption to begin with?</p> <p>In order to simplify the world a little, we assume that at any given moment the state of the environment represents everything that we need to know in order to act. What happened before with the environment, our (the agent's) past mistakes, nothing matters. Wanted to use here \"carpe diem\", yet the expression is not appropriate as the future is very relevant. You do now what is right to do to the best of your policy, yet of course you need to take into account the future, and try to foresee it, and what will be the consequences of your current action. Regrets are not helpful, but planning of some sort is the essence of RL.</p> <p>The environment, if indeed adheres to the Markov property, behaves, subject to randomness, similarly when we return to the same state, and take the same action. This is exactly what an agent needs to learn. As we associate the reward mechanism with the environment, and we assume Markov property for the next state as well as for the reward, we can refer to the environment as a Markov Decision Process (MDP), which is exactly that, an environment with some dynamics that depends only on the current state, and the action the agent shall take. Note that the environment is often stochastic, and this does not contradic it being an MDP.</p> <p>So, back to the question, did we break the Markov property assumption by introducing history?</p> <p>By changing the observation space, we've created a new MDP, for which there are more states. So still an MDP but a more detailed one. That engineering intervention can potentially help or it might harm the learning process. Yet to experiment.</p>"},{"location":"Gymnasium/wrappers/#rewardwrapper","title":"RewardWrapper","text":"<p>A third useful base class wrapper is RewardWrapper. Why to touch the reward? Maybe a reward of +1/-1 at the end of the game is correct, but it takes the agent a lot of time to get to a win, and on the way getting no signal for progress is not helpful. We can add indication of good progress by giving some small rewards signals on the way. This shall be under reward shaping, which is a very important concept. Another reason that we may want to change the reward is in order to scale / normalize it for the benefit of our neural network learning optimization.</p>"},{"location":"PettingZoo/pettingzoo/","title":"PettingZoo","text":"<p>PettingZoo is the multi agent (MA) \"extension\" of Gym/Gymnasium. For example for a Chess environment, maybe we organize ML competitions, and every submission is an agent, then we match agents and a board. Both agents should interact with the environment, each at their turn.</p> <p>Maybe the game is simultaneous, like \"Rock - Paper - Scissors\". Both players place an action, only then both players get to see the outcome.</p> <p>Another example for a MA environment can be a football game, where we have an agent for each of the player-robots. Even when we think of the solution as a single brain that controls all of its team players, it may still make sence to have a per-player action.</p> <p>And so, with PettingZoo MA is doable. A PettingZoo environment is not a Gymnasium environment, as the interface is a a bit different, yet the differences are simple to follow. Just for illustration, and not necessarily what was choosen, consider for example, that instead of passing a single action to a 'step' function, one may need to pass a list of actions, or call 'step' multiple times, each time with an action per the current agent in a loop. We'll see the actual interface next.</p> <p>PettingZoo makes Gymnasium its dependancy. Among other stuff, it takes from Gymnasium the definitions for the spaces.</p> <p>There are two flavors from PettingZoo environments: Agent Environment Cycle (AEC) API, and Parallel API.</p>"},{"location":"PettingZoo/pettingzoo/#agent-environment-cycle-aec-api","title":"Agent Environment Cycle (AEC) API","text":"<p>Below is an example from PettingZoo documentation.</p> <pre><code>from pettingzoo.classic import chess_v5\nenv = chess_v5.env(render_mode=\"human\")\nenv.reset()\nfor agent in env.agent_iter():\nobservation, reward, termination, truncation, info = env.last()\nif termination or truncation:\naction = None\nelse:\naction = env.action_space(agent).sample(observation[\"action_mask\"])  # this is where you would insert your policy\nenv.step(action)\nenv.close()\n</code></pre> <p>We note that at line 5 we have an iteration over the current agents. The iteration value is a string representing the agent (its identification). This identification is used later to access, for example the action space (line 10). When we 'step' at line 11 we provide the action of the current agent. We also see at line 6 that a call to 'env.last()' should return what the current agent sees (after a call for 'reset' was done done on the environment or another agent has just stepped).</p> <p>We also see that for that specific environment 'chess_v5', an observation is a kind of a dict that has also the 'action_mask' key (line 10). For more information about 'action_mask' see action masking.</p> <p>Note that as of the iteration, we may see the for example the following sequence: A, B, C, A, B, C, A, B, A, B, D, .. ('C' dissapeared after the second round, 'D' appeared at a later stage). For Chess we probably just have player_0, player_1, player_0, etc..</p>"},{"location":"PettingZoo/pettingzoo/#parallel-api","title":"Parallel API","text":"<p>For a parallel environment, at each round there are specific agents that need to act. All those agents act simultaneously. In next round we may see a different set of agents.</p> <p>Here is the example from the documentation.</p> <pre><code>from pettingzoo.butterfly import pistonball_v6\nparallel_env = pistonball_v6.parallel_env()\nobservations = parallel_env.reset()\nwhile env.agents:\nactions = {agent: parallel_env.action_space(agent).sample() for agent in parallel_env.agents}  # this is where you would insert your policy\nobservations, rewards, terminations, truncations, infos = parallel_env.step(actions)\n</code></pre>"},{"location":"PettingZoo/pettingzoo/#implementing-a-pettingzoo-environment","title":"Implementing a PettingZoo Environment","text":"<p>If you want to implement a PettingZoo anvironment, it is a bit more involved than implementing a Gymnasium environment, as you need, for example, to keep the results of a 'step' call for a later 'last' call (for an AEC environment), or to maintain a mapping for an agent identity to an action_space (by implementing the 'action_space' function that takes an agent identity as an argument).</p> <p>On the other hand, if you do have a turns-play \"environment\" it may make more sense to implement if first as a PettingZoo environment, even if you want to train a single agent. You can then \"wrap\" the MA environment and all the other agents, into a Gymnasium environment, and expose only what your agent needs to know and interact with. I've done a similar thing, I'm going to describe it in Wrapping a PettingZoo into a Gymnasium one. I've provided there \"wrappers\" that make your PettingZoo environment into a simple Gymnasium environment, to be used for example with Stable-Baselines3.</p>"},{"location":"PettingZoo/pettingzoo/#supporting-agentsalgorithms-libraries","title":"Supporting Agents/Algorithms Libraries","text":"<p>Stable Baselines3 (SB3), is a go to package for Gymnasium environment. You can quickly try to train an agent to interact with an environment. SB3 is really simple and that would be my first go to. SB3 is kind of the 'scikit-learn' for RL. Unfortunatly, as far as I know SB3 does not support PettingZoo.</p> <p>One good option to have a quick try with PettingZoo environment is to use Tianshou. I've tried Tianshou and it worked nice for me. It has a class MultiAgentPolicyManager that basically wraps one or more agent, and hence a policy of type Tianshou MultiAgentPolicyManager can interact with a PettingZoo environment.</p> MARL following Tainshou (with MultiAgentPolicyManager) <p>Tianshou is a little more involved than SB3, yet still reasonable. I plan to give a try also to RLLib, which is what a lot of people used those days. On my todo list... Please note that with Tianshou MultiAgentPolicyManager we have a collection of agents, each with its own policy, and so only the (PettingZoo) environment is shared. There are algorithms that do want us to share some common \"top-level\" information among our cooperating agents.</p> <p>As I've mentioned before that I've implemented a wrapping mechanism that turns a PettingZoo environment into a Gymnasium one. This can be used when you have already an implementation of a Chess player for example, or maybe want to beat a random agent, and then maybe bootstrap each round of a turnament againt the previous version of your agent (self-play). With the PettingZoo to Gymnasium wrapper functionality available in quertyenv, one can even use SB3. My \"trick\" was to provide in the initialization of the environment a callable that is basically the implementation of the other agent(s) \"predict\" method (AKA \"act\" method).</p>"},{"location":"PettingZoo/wrapping_pz_into_gym/","title":"Wrapping a PettingZoo into a Gymnasium one","text":"<p>In order to have a PettingZoo environment presenting itself as a Gymnasium environment, we need to know which of the agents is the \"external agent\", the one that should not be \"wrapped into\" the Gymnasium environment. In addition we need to know how to get the actions for all other agents.</p> <p>In quertyenv, I have implemented the 'aec_to_gymnasium' and the 'parallel_to_gymnasium' functions:</p> <pre><code>from typing import Any, Callable, Dict, Optional\nimport gymnasium as gym\nfrom gymnasium.core import ActType, ObsType\nfrom pettingzoo import AECEnv, ParallelEnv\n# The first parameter is an agent (its identification),\n#  second parameter is the relevant observation.\n# The callable is expected to return the action that the\n#  given agent would like to take.\nActOthers = Callable[[str, ObsType], ActType]\ndef aec_to_gymnasium(aec_env: AECEnv,\nexternal_agent: str, act_others: ActOthers):\n\"\"\"Makes a Gymnasium environment out of a AECEnv.\n    ...\n    \"\"\"\n...\ndef parallel_to_gymnasium(parallel_env: ParallelEnv,\nexternal_agent: str, act_others: ActOthers):\n\"\"\"Makes a Gymnasium environment out of a ParallelEnv.\n    ...\n    \"\"\"\n...\n</code></pre> <p>I'll describe briefly the implementation. You can check the code for yourself there. For both functions, I return an instance of a (inner) class defined in the function itself <code>class WrapperEnv(gym.Env)</code>.  'aec_to_gymnasium' was a bit trickier. I had to check if the current agent is the 'external_agent' or is it another agent. I used the following member function of my 'WrapperEnv' class.</p> part of (inner) class WrapperEnv(gym.Env) - aec_to_gymnasium<pre><code>def _loop_others(self):\nfor agent in self._aec_env.agent_iter():\nif agent == self._external_agent:\nbreak\nobservation, _, terminated, truncated, _ = self._aec_env.last()\nif terminated or truncated:\nbreak\naction_current = self._act_others(agent, observation)\nself._aec_env.step(action_current)\n</code></pre> <p>For 'parallel_to_gymnasium' it was just a question of where to take the next action, from the provided argument to the 'step' method, or from 'act_others' callable that was provieded in the initialization.</p> part of (inner) class WrapperEnv(gym.Env) - parallel_to_gymnasium<pre><code>def step(self, action):\n...\nactions = {\nagent: (\naction\nif agent == self._external_agent\nelse self._act_others(agent, self._observations[agent])\n)\nfor agent in self._parallel_env.agents\n}\n...\n</code></pre> <p>An example for the usage of those functions, is as follows:</p> <pre><code>def test_tictactoe(me, other):\naec_env = tictactoe_v3.env()\ndef pick_a_free_square(obs):\naction_mask = obs[\"action_mask\"]\npossible_actions = np.where(action_mask == 1)[0]\nreturn np.random.choice(possible_actions)\nother_agents_logic = {other: pick_a_free_square}\ngym_env = aec_to_gymnasium(\naec_env=aec_env,\nexternal_agent=me,\nact_others=(\nlambda agent, observation: other_agents_logic[agent](observation)\n),\n)\n...\n</code></pre> <p>In above example, the lambda function 'act_others' is making a use of a dict 'other_agents_logic'. In this simple case, we could also just ignore the 'agent' argument, and call 'pick_a_free_square' directly.</p>"},{"location":"concepts/action_masking/","title":"Action Masking","text":"<p>When we define an action space for the environment we develop, we set the rules by which an agent can influence its own destiny in the environment. This action space should match the capabilities of the agent and should be relevant for the environment.</p> <p>For example, if the observation space is the cell\u2019s cartesian coordinates (x, y) in a maze where the agent finds itself, then the action space can be, as an example, {up, right, down, left}. It is convenient for the next steps in implementing the agent, that the action space is simple and fixed across all the different observation, which lends itself to a simpler deep neural network architecture. For example one with a fixed-sized output layer of action probabilities.</p> <p>An issue arise when in different situations there are illegal moves. Now we are left to solve the situation in which the agent wants us to communicate to the environment an illegal action. Imagine an illegal Chess move such as moving the agent's Rock to the square where its second Rock is placed. How to address this scenario?</p> <p>A possible solution in such a situation would be to terminate an episode, giving the agent a big negative reward. We hope that the agent learns to avoid illegal moves. A Gym wrapper can be developed that simply ignores an invalid action, bypasses the original environment, and just returns the current observation, the negative reward, and the terminated flag. This may work eventually yet it is somewhat wasteful in the sense that we are in a position to communicate to the agent the rules of the game (environment). This may be somewhat an abuse of AI. An analogy would be to train an agent to multiply integers using supervised learning techniques. We\u2019ll wind up with less than perfect capable agent, after having invested resources on things that should be addressed much simpler. On the other hand the agent may learn to avoid being stuck in a corner, as an example. So keep the option of returning a negative reward and terminating, also in your mind when designing an RL environment / agents.</p> <p>Another option that comes to mind is to ask the agent for an alternative action, or to completely ignore the agent\u2019s wish and take a random (legal) action if the agent fails to provide a valid action. For example use Gym ActionWrapper to achieved above. Care should be taken to communicate the action that was actually taken to the trainer so that the right conclusion shall be drawn from the relevant  interaction of the agent in the environment. The risk with this approach is that we protect our \u201cbaby\u201d agent from facing the real world, which may have consequences of its ability to learn sophisticated behaviors.</p> <p>Eventually we are getting to action masking. The idea is that the environment, or a wrapper of the environment shall communicate to the agent what are the possible moves when the agent is about to take an action. So while the agent is born with the capabilities to act for example {up, right, down, left}, it shall be told, \"but now you should only choose from {right, left}\". In order to achieve this functionality we need both the environment or a wrapper of it to add the legal moves to the observation, and also we need our implementation to be able to take this additional information. For example as an expected optional field in the observation dictionary (as is done in Tianshou), or as a extra parameter for the act / predict / forward call.</p> <p>What happens behind the scenes? One way to address it in the RL library is to combine the mask with the outputs and to make the illegal moves get (almost) zero probability. The property of differential loss is kept and the learning by stochastic decent or by its variant optimization techniques is used. There are additional ways to address that. See for example the following research / survey paper [1]. </p>"},{"location":"concepts/action_masking/#references","title":"References","text":"<p>[1] A Closer Look at Invalid Action Masking in Policy Gradient Algorithms.</p>"}]}